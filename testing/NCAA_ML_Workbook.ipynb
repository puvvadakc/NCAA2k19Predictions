{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:426: FutureWarning: You should specify a value for 'n_splits' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "resultsWin = pd.read_csv('data/RegularSeasonDetailedResults.csv')\n",
    "teams = pd.read_csv('data/Teams.csv')\n",
    "\n",
    "# Get only winning teams game stats to predict their score\n",
    "resultsWin = resultsWin.drop(['WTeamID', 'LTeamID', 'WLoc'], axis=1)\n",
    "\n",
    "def neuralNetwork(results) :\n",
    "    train_features, test_features, train_outcome, test_outcome = train_test_split(\n",
    "        results.drop(\"WScore\", axis=1),\n",
    "        results.WScore,\n",
    "        test_size=0.30, \n",
    "        random_state=11\n",
    "    )\n",
    "    scaler = MinMaxScaler()\n",
    "    mlp_reg = MLPClassifier()\n",
    "\n",
    "    imputer = Imputer()\n",
    "    selector = SelectPercentile()\n",
    "    threshold = VarianceThreshold(.1)\n",
    "    pipe = make_pipeline(imputer, threshold, selector, scaler, mlp_reg)\n",
    "\n",
    "    param_grid = {\n",
    "        'selectpercentile__percentile':range(10, 30, 5)\n",
    "        }\n",
    "\n",
    "    crossVal = KFold()\n",
    "    grid = GridSearchCV(pipe, param_grid, cv = crossVal, scoring=\"neg_mean_absolute_error\")\n",
    "    grid.fit(train_features, train_outcome)\n",
    "    grid.score(test_features, test_outcome)\n",
    "\n",
    "    score = grid.score(test_features, test_outcome)\n",
    "\n",
    "    predictedValues = grid.predict(test_features)\n",
    "\n",
    "    return [score, predictedValues, grid, test_outcome]\n",
    "\n",
    "neural = neuralNetwork(resultsWin)\n",
    "\n",
    "# How many points off were we from predicting the winning score?\n",
    "print(neural[0])\n",
    "\n",
    "# This can be exactly replicated for the other team using LScore for losing score\n",
    "# Then need to randomize which teams get which grid and we run the grid for each playoff game invididually\n",
    "# in their own csv. Take the two scores, see who won, move them manually to the next round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsWin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not quite done withe the following but it is a way to predict the outcomes of the tournament games as they contunue. The games each round can be based off of the predicted outcome of the last round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_round(teams_df):\n",
    "    '''Takes a dataframe with two columns: School id, and ranking, none of \n",
    "    these teams should have been eliminated. It returns the next matchups for these \n",
    "    teams based on their ranking'''\n",
    "    arar = np.char.array(['01','16','08','09','05','12','04','13','06','11','03','14','07','10','02','15'])\n",
    "    arr = np.append(arar, arar)\n",
    "    first_round_bracket = np.char.array(['W', 'X', 'Y', 'Z']).repeat(16) + np.append(arr, arr)\n",
    "    if(len(teams_df) < 64):\n",
    "        won_ranks = teams_df['Seed'].values\n",
    "        first_round_bracket = np.array([x for x in first_round_bracket if x in won_ranks])\n",
    "    #print(first_round_bracket)\n",
    "    picks1 = first_round_bracket[np.arange(1, len(teams_df), 2)]\n",
    "    picks2 = first_round_bracket[np.arange(0, len(teams_df), 2)]\n",
    "    #print(picks1)\n",
    "    #print(picks2)\n",
    "    teams_df = teams_df.set_index('Seed')\n",
    "    teams_next = pd.DataFrame()\n",
    "    teams_next['team1'] = teams_df.loc[picks1, 'TeamID'].values\n",
    "    teams_next['team2'] = teams_df.loc[picks2, 'TeamID'].values\n",
    "    teams_next['rank1'] = picks1\n",
    "    teams_next['rank2'] = picks2\n",
    "    return teams_next\n",
    "\n",
    "def find_winners(nx):\n",
    "    '''Takes a dataframe with teamsids and ranks from matches and keeps only the \n",
    "    winners'''\n",
    "    nx['score'] = (nx['team1'] - nx['team2'])\n",
    "    nx['TeamID'] = nx.loc[:, 'team2']\n",
    "    nx['Seed'] = nx.loc[:, 'rank2']\n",
    "    \n",
    "    condition = nx['score'] > 0\n",
    "    #this can be replaced with an ouput from a neural net to predict winners\n",
    "    underdogs = nx.loc[condition,['rank1', 'team1']]\n",
    "    underdogs.columns = ['Seed', 'TeamID']\n",
    "    nx.update(underdogs)\n",
    "    return(nx)#.loc[:, ['Seed', 'TeamID']])\n",
    "\n",
    "def first_four(teams_df, games_record):\n",
    "    pregames = teams_df.loc[teams_df['Seed'].str.contains('a|b'),:]\n",
    "    teams_df = teams_df.loc[~teams_df['Seed'].str.contains('a|b'),:]\n",
    "    #features = pd.merge(games, team_summary_stats, how='left', left_on=['team1'], right_on=['TeamID'])\n",
    "    #features = pd.merge(games, team_summary_stats, how='left', left_on=['team2'], right_on=['TeamID'], suffixes=('', '_t2'))\n",
    "    teams_next = pd.DataFrame()\n",
    "    teams_next['team1'] = pregames.iloc[np.arange(1, len(pregames), 2), 1].values\n",
    "    teams_next['team2'] = pregames.iloc[np.arange(0, len(pregames), 2), 1].values\n",
    "    teams_next['rank1'] = pregames.iloc[np.arange(1, len(pregames), 2), 0].values\n",
    "    teams_next['rank2'] = pregames.iloc[np.arange(0, len(pregames), 2), 0].values\n",
    "    to_begin = find_winners(teams_next.copy())\n",
    "    to_begin['round'] = 0\n",
    "    if len(games_record) < 1:\n",
    "        games_record = to_begin.copy()#.loc[:,:]\n",
    "    else:\n",
    "        games_record = games_record.append(to_begin.copy(), ignore_index=True)\n",
    "    to_begin['Seed'] = [x[0:-1] for x in to_begin['Seed'].values]\n",
    "    teams_df = teams_df.append(to_begin.loc[:, ['Seed', 'TeamID']])\n",
    "    #print(to_begin.loc[:, ['Seed', 'TeamID']])\n",
    "    return([teams_df, games_record])\n",
    "\n",
    "games_record = pd.DataFrame() # records all matches\n",
    "teams_df = pd.read_csv('data/NCAATourneySeeds.csv')\n",
    "teams_df = teams_df.loc[teams_df.Season == 2003, ['Seed', 'TeamID']]\n",
    "teams_df, games_record = first_four(teams_df, games_record)\n",
    "#print(teams_df)\n",
    "round = 1 \n",
    "\n",
    "#runs until only 1 team remains\n",
    "while len(teams_df) > 1:\n",
    "    games = next_round(teams_df)\n",
    "    features = pd.merge(games, team_summary_stats, how='left', left_on=['team1'], right_on=['TeamID'])\n",
    "    features = pd.merge(features, team_summary_stats, how='left', left_on=['team2'], right_on=['TeamID'], suffixes=('', '_t2'))\n",
    "    print(features.head(2))\n",
    "    games['round'] = round\n",
    "    teams_df = find_winners(games)\n",
    "    if len(games_record) < 1:\n",
    "        games_record = games#.loc[:,:]\n",
    "    else:\n",
    "        games_record = games_record.append(games, ignore_index=True)\n",
    "    round = round + 1\n",
    "#print(games_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_season_results = pd.read_csv('data/RegularSeasonDetailedResults.csv')\n",
    "post_season_outcomes = pd.read_csv('data/NCAATourneyDetailedResults.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a set of regular season features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Season  TeamID        FGM        FGA      FGM3       FGA3        FTM  \\\n",
      "0    2003    1102  19.142857  39.785714  7.821429  20.821429  11.142857   \n",
      "1    2003    1103  27.148148  55.851852  5.444444  16.074074  19.037037   \n",
      "2    2003    1104  24.035714  57.178571  6.357143  19.857143  14.857143   \n",
      "3    2003    1105  24.384615  61.615385  7.576923  20.769231  15.423077   \n",
      "4    2003    1106  23.428571  55.285714  6.107143  17.642857  10.642857   \n",
      "\n",
      "         FTA         OR         DR        Ast         TO       Stl       Blk  \\\n",
      "0  17.107143   4.178571  16.821429  13.000000  11.428571  5.964286  1.785714   \n",
      "1  25.851852   9.777778  19.925926  15.222222  12.629630  7.259259  2.333333   \n",
      "2  20.928571  13.571429  23.928571  12.107143  13.285714  6.607143  3.785714   \n",
      "3  21.846154  13.500000  23.115385  14.538462  18.653846  9.307692  2.076923   \n",
      "4  16.464286  12.285714  23.857143  11.678571  17.035714  8.357143  3.142857   \n",
      "\n",
      "          PF  \n",
      "0  18.750000  \n",
      "1  19.851852  \n",
      "2  18.035714  \n",
      "3  20.230769  \n",
      "4  18.178571  \n"
     ]
    }
   ],
   "source": [
    "winners = regular_season_results.loc[:,['Season', 'WTeamID', 'WFGM', 'WFGA', 'WFGM3', 'WFGA3', \n",
    "                                         'WFTM', 'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF']]\n",
    "losers = regular_season_results.loc[:,['Season', 'LTeamID', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
    "                                      'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']]\n",
    "winners.columns = ['Season', 'TeamID', 'FGM', 'FGA', 'FGM3', 'FGA3',\n",
    "                                      'FTM', 'FTA', 'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF']\n",
    "losers.columns = ['Season', 'TeamID', 'FGM', 'FGA', 'FGM3', 'FGA3',\n",
    "                                      'FTM', 'FTA', 'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF']\n",
    "all_teams = winners.copy()\n",
    "all_teams = all_teams.append(losers.copy(), ignore_index=True)\n",
    "team_summary_stats = all_teams.groupby(['Season', 'TeamID'], as_index=False).mean()\n",
    "print(team_summary_stats.head())\n",
    "team_summary_stats.to_csv('data/team_summary_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a set of post season outcomes to test with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_season_outcomes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Season  WTeamID  LTeamID  score_dif\n",
      "0    2003     1421     1411          8\n",
      "1    2003     1112     1436         29\n",
      "2    2003     1113     1272         13\n",
      "3    2003     1141     1166          6\n",
      "4    2003     1143     1301          2\n",
      "   Season  WTeamID  LTeamID  score_dif\n",
      "0    2003     1421     1411          8\n",
      "1    2003     1436     1112        -29\n",
      "2    2003     1272     1113        -13\n",
      "3    2003     1141     1166          6\n",
      "4    2003     1143     1301          2\n",
      "1048\n",
      "1048\n",
      "1048\n",
      "    Season  WTeamID  LTeamID  score_dif  TeamID       FGM        FGA  \\\n",
      "0     2003     1421     1411          8    1421 -0.354023   1.526437   \n",
      "1     2003     1421     1400        -21    1421 -3.620690  -5.635468   \n",
      "2     2003     1277     1400         -9    1277 -4.967742 -11.331797   \n",
      "3     2003     1345     1400        -10    1345 -4.250000  -8.035714   \n",
      "4     2003     1163     1400         -4    1163  1.533333  -0.228571   \n",
      "5     2003     1436     1112        -29    1436 -5.493842  -9.852217   \n",
      "6     2003     1242     1112          3    1242 -0.088095  -3.414286   \n",
      "7     2003     1323     1112        -17    1323 -3.095622  -5.036866   \n",
      "8     2003     1272     1113        -13    1272 -0.931034   3.103448   \n",
      "9     2003     1242     1113         32    1242  3.026437   5.403448   \n",
      "10    2003     1141     1166          6    1141 -2.076280  -4.764890   \n",
      "11    2003     1143     1301          2    1143  3.011494   5.390805   \n",
      "12    2003     1143     1328         -9    1143  2.078161   2.190805   \n",
      "13    2003     1140     1163         -5    1140 -5.501075 -10.941935   \n",
      "14    2003     1390     1163        -11    1390 -3.888172  -4.167742   \n",
      "\n",
      "        FGM3      FGA3       FTM       FTA        OR        DR       Ast  \\\n",
      "0   0.549425 -0.500000 -1.434483 -7.135632 -0.890805 -1.627586 -1.165517   \n",
      "1   0.625616  1.214286 -1.034483 -2.854680 -3.902709 -2.970443 -1.465517   \n",
      "2  -0.953917 -3.398618 -0.741935 -1.656682 -5.501152 -1.884793 -0.887097   \n",
      "3  -0.750000 -1.285714  2.142857  1.714286 -5.785714 -2.678571 -2.250000   \n",
      "4   0.209524 -1.085714 -2.100000 -1.685714 -1.411905  1.757143  1.133333   \n",
      "5  -1.759852 -4.588670 -4.673645 -5.448276 -2.213054 -1.918719 -3.435961   \n",
      "6  -2.235714 -5.938095 -1.469048 -0.866667 -0.878571 -0.742857 -0.909524   \n",
      "7   1.254608  1.702765 -0.180876 -2.161290 -3.791475 -0.771889 -0.739631   \n",
      "8   3.000000  7.482759 -2.586207 -3.310345  0.379310  2.655172  1.068966   \n",
      "9   0.800000  1.547126 -1.485057 -2.073563  0.610345  3.589655  1.181609   \n",
      "10 -1.142111 -2.553814  5.397074  5.142111 -0.292581  0.094044 -1.197492   \n",
      "11 -1.552874 -5.465517 -2.387356 -0.949425  1.508046  2.345977  1.333333   \n",
      "12 -1.052874 -1.932184  0.212644  0.917241 -0.891954 -0.587356  1.833333   \n",
      "13  0.126882  0.429032  3.293548  2.061290 -3.895699 -3.480645 -2.213978   \n",
      "14  0.868817  3.687097 -0.867742 -1.325806 -1.024731 -2.770968 -0.988172   \n",
      "\n",
      "          TO       Stl       Blk        PF  \n",
      "0   0.973563  0.635632  0.766667  0.803448  \n",
      "1   2.778325  0.676108 -0.857143 -1.253695  \n",
      "2   1.119816 -0.005760 -0.147465 -0.324885  \n",
      "3   0.392857  0.785714 -0.928571 -0.535714  \n",
      "4   2.371429 -0.459524  3.876190 -1.957143  \n",
      "5  -0.716749 -1.602217 -1.248768 -1.853448  \n",
      "6   0.114286  1.669048  0.685714 -1.050000  \n",
      "7  -2.011521 -1.012673  1.430876 -1.524194  \n",
      "8  -0.206897  2.172414  0.827586 -0.655172  \n",
      "9   0.900000  4.926437  0.658621 -2.713793  \n",
      "10  4.877743 -1.290491 -0.454545  3.692790  \n",
      "11 -0.027586 -1.214943 -0.273563 -1.563218  \n",
      "12  2.372414 -0.381609 -0.973563 -1.496552  \n",
      "13 -2.058065  1.002151 -5.217204  3.019355  \n",
      "14 -1.477419 -0.417204 -4.378495  0.116129  \n",
      "    Season  TeamID        FGM        FGA      FGM3       FGA3        FTM  \\\n",
      "0     2003    1102  19.142857  39.785714  7.821429  20.821429  11.142857   \n",
      "1     2003    1103  27.148148  55.851852  5.444444  16.074074  19.037037   \n",
      "2     2003    1104  24.035714  57.178571  6.357143  19.857143  14.857143   \n",
      "3     2003    1105  24.384615  61.615385  7.576923  20.769231  15.423077   \n",
      "4     2003    1106  23.428571  55.285714  6.107143  17.642857  10.642857   \n",
      "5     2003    1107  24.035714  57.464286  7.928571  22.178571   9.928571   \n",
      "6     2003    1108  24.939394  58.727273  5.212121  16.333333  14.000000   \n",
      "7     2003    1110  23.166667  53.533333  8.766667  23.133333  11.266667   \n",
      "8     2003    1111  28.615385  64.500000  7.192308  21.961538  18.846154   \n",
      "9     2003    1112  30.321429  65.714286  7.035714  20.071429  17.535714   \n",
      "10    2003    1113  27.206897  56.896552  4.000000  12.586207  17.551724   \n",
      "11    2003    1114  23.857143  53.214286  6.785714  18.285714  15.535714   \n",
      "12    2003    1115  19.250000  54.285714  4.750000  16.857143  11.250000   \n",
      "13    2003    1116  21.821429  55.571429  6.071429  19.571429  11.928571   \n",
      "14    2003    1117  23.692308  54.692308  7.923077  21.346154  15.807692   \n",
      "\n",
      "          FTA         OR         DR        Ast         TO       Stl       Blk  \\\n",
      "0   17.107143   4.178571  16.821429  13.000000  11.428571  5.964286  1.785714   \n",
      "1   25.851852   9.777778  19.925926  15.222222  12.629630  7.259259  2.333333   \n",
      "2   20.928571  13.571429  23.928571  12.107143  13.285714  6.607143  3.785714   \n",
      "3   21.846154  13.500000  23.115385  14.538462  18.653846  9.307692  2.076923   \n",
      "4   16.464286  12.285714  23.857143  11.678571  17.035714  8.357143  3.142857   \n",
      "5   13.535714   8.250000  20.250000  11.928571  12.571429  6.857143  2.035714   \n",
      "6   20.939394  13.121212  23.212121  13.848485  18.454545  8.181818  3.515152   \n",
      "7   16.533333  10.000000  23.133333  14.233333  13.166667  6.400000  1.733333   \n",
      "8   26.192308  13.000000  25.307692  15.230769  16.192308  9.846154  4.615385   \n",
      "9   25.000000  15.178571  27.642857  17.642857  14.785714  8.464286  4.214286   \n",
      "10  26.206897  13.689655  23.310345  15.551724  14.000000  5.206897  4.241379   \n",
      "11  22.428571  10.535714  22.857143  11.500000  15.964286  6.607143  2.357143   \n",
      "12  17.607143   9.964286  21.035714  10.178571  19.107143  7.964286  2.750000   \n",
      "13  19.571429  14.535714  23.428571   9.821429  16.678571  6.464286  3.535714   \n",
      "14  21.423077  10.961538  20.384615  11.923077  13.269231  7.153846  1.653846   \n",
      "\n",
      "           PF  \n",
      "0   18.750000  \n",
      "1   19.851852  \n",
      "2   18.035714  \n",
      "3   20.230769  \n",
      "4   18.178571  \n",
      "5   15.892857  \n",
      "6   19.666667  \n",
      "7   18.133333  \n",
      "8   18.692308  \n",
      "9   17.750000  \n",
      "10  19.413793  \n",
      "11  20.500000  \n",
      "12  20.250000  \n",
      "13  20.928571  \n",
      "14  21.423077  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "post_season_outcomes['score_dif'] = post_season_outcomes.WScore - post_season_outcomes.LScore\n",
    "outcome = post_season_outcomes.loc[:,['Season', 'WTeamID', 'LTeamID', 'score_dif']]\n",
    "mixing_matrix = np.random.choice([True, False], len(outcome))\n",
    "mixed_outcome = outcome.copy()\n",
    "print(mixed_outcome.head())\n",
    "mixed_outcome.loc[mixing_matrix, ['WTeamID', 'LTeamID']] = mixed_outcome.loc[mixing_matrix, ['LTeamID', 'WTeamID']].values \n",
    "mixed_outcome.loc[mixing_matrix, ['score_dif']] = mixed_outcome.loc[mixing_matrix, ['score_dif']].mul(-1)\n",
    "print(mixed_outcome.head())\n",
    "print(len(mixed_outcome))\n",
    "features = pd.merge(mixed_outcome, team_summary_stats, left_on=['WTeamID', 'Season'], right_on=['TeamID', 'Season'])\n",
    "print(len(features))\n",
    "features = pd.merge(features, team_summary_stats, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], suffixes=('', '_t2'))\n",
    "print(len(features))\n",
    "features.loc[:, 'FGM':'PF'] = features.loc[:, 'FGM':'PF'].values - features.loc[:, 'FGM_t2':'PF_t2'].values\n",
    "features = features.loc[:, 'Season':'PF']\n",
    "features.to_csv('data/mixed_prepared_features.csv')\n",
    "print(features.head(15))\n",
    "print(team_summary_stats.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A possible model for predicting games based on regular season summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ModelFunctions import DecisionTreeFunc, KnnFunc, BayesianRidge, NeuralNetworkFunc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore') # disables warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#features.score_dif = 1*(features.score_dif > 0)\n",
    "test_year = features.loc[features.Season == 2018, :]\n",
    "test_features = features.loc[features.Season != 2018, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetworkFunc(test_features.drop(columns=['score_dif', 'Season']), test_features.score_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.94563819,  -2.70618897,  -4.73145021,   1.58826746,\n",
       "         5.59134072,   3.10472827,   4.61348369,   0.28999883,\n",
       "        -1.94011455,  -9.92342847,  -4.5571513 , -10.75685272,\n",
       "        -8.0856508 ,   8.46420474,   2.88282002,  -8.28192799,\n",
       "        -3.10287793,   7.14973875,   3.96131448,   6.21057926,\n",
       "         1.90678681,   2.30675634,  -1.75695709,  -2.31971788,\n",
       "        -0.05710476,  -4.35365404,  -1.00688445,   3.84558418,\n",
       "         5.14461251,   5.38719835,   0.43983968,   5.0326159 ,\n",
       "         0.28233775,  -4.80553825,  -0.74052514,  -2.04909353,\n",
       "         2.52973579,   6.43828148,  -5.83958416, -13.55740855,\n",
       "        -7.29226791, -11.73713516, -12.0000586 ,   1.97645917,\n",
       "        -1.60048812,   8.74034219,  -0.49399481,   5.19519106,\n",
       "         1.33819201,  -3.42069667,   6.04765411,   5.02512541,\n",
       "        -2.21656216,   5.81959942,  -1.57232255,  15.19533451,\n",
       "        -5.59740665,   3.84891856,  -5.64378664,  -2.19932379,\n",
       "         4.64555379,   4.71243389,   2.72313461,   0.40270997,\n",
       "         6.16589505,   0.59187524,   1.58663749])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_year.drop(columns=['score_dif', 'Season']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Season  WTeamID  LTeamID  score_dif  TeamID       FGM        FGA      FGM3  \\\n",
      "0    2003     1421     1411          8    1421 -0.354023   1.526437  0.549425   \n",
      "1    2003     1421     1400        -21    1421 -3.620690  -5.635468  0.625616   \n",
      "2    2003     1277     1400         -9    1277 -4.967742 -11.331797 -0.953917   \n",
      "3    2003     1345     1400        -10    1345 -4.250000  -8.035714 -0.750000   \n",
      "4    2003     1163     1400         -4    1163  1.533333  -0.228571  0.209524   \n",
      "\n",
      "       FGA3       FTM       FTA        OR        DR       Ast        TO  \\\n",
      "0 -0.500000 -1.434483 -7.135632 -0.890805 -1.627586 -1.165517  0.973563   \n",
      "1  1.214286 -1.034483 -2.854680 -3.902709 -2.970443 -1.465517  2.778325   \n",
      "2 -3.398618 -0.741935 -1.656682 -5.501152 -1.884793 -0.887097  1.119816   \n",
      "3 -1.285714  2.142857  1.714286 -5.785714 -2.678571 -2.250000  0.392857   \n",
      "4 -1.085714 -2.100000 -1.685714 -1.411905  1.757143  1.133333  2.371429   \n",
      "\n",
      "        Stl       Blk        PF  \n",
      "0  0.635632  0.766667  0.803448  \n",
      "1  0.676108 -0.857143 -1.253695  \n",
      "2 -0.005760 -0.147465 -0.324885  \n",
      "3  0.785714 -0.928571 -0.535714  \n",
      "4 -0.459524  3.876190 -1.957143  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('mlpregressor', MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_...=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None, param_grid={},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "mlp_reg = MLPRegressor()\n",
    "print(features.head())\n",
    "param_grid = {}#'mlpregressor__activation' : ['identity', 'logistic', 'tanh', 'relu']}\n",
    "#print(test_year.score_dif)\n",
    "pipe = make_pipeline(scaler, mlp_reg)\n",
    "grid = GridSearchCV(pipe, param_grid)\n",
    "grid.fit(test_features.drop(columns=['score_dif', 'Season']), test_features.score_dif)\n",
    "#print(grid.score(test_year.drop(columns=['score_dif', 'Season']), test_year.score_dif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real</th>\n",
       "      <th>predicted</th>\n",
       "      <th>FGAdif</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>-10</td>\n",
       "      <td>-2.823683</td>\n",
       "      <td>3.519886</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>7</td>\n",
       "      <td>-3.436291</td>\n",
       "      <td>-3.531250</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>-15</td>\n",
       "      <td>-3.480578</td>\n",
       "      <td>-2.687500</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>-4</td>\n",
       "      <td>2.259196</td>\n",
       "      <td>5.833822</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>-2</td>\n",
       "      <td>4.966380</td>\n",
       "      <td>2.303030</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>18</td>\n",
       "      <td>4.172597</td>\n",
       "      <td>1.596774</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>-3</td>\n",
       "      <td>6.553689</td>\n",
       "      <td>1.387868</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>-21</td>\n",
       "      <td>3.257656</td>\n",
       "      <td>-7.758467</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>20</td>\n",
       "      <td>0.333798</td>\n",
       "      <td>-7.199643</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>-22</td>\n",
       "      <td>-8.000342</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>4</td>\n",
       "      <td>-3.250805</td>\n",
       "      <td>-1.409982</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>-4</td>\n",
       "      <td>-9.588147</td>\n",
       "      <td>-7.515152</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>-25</td>\n",
       "      <td>-7.330038</td>\n",
       "      <td>-0.814394</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>4</td>\n",
       "      <td>8.564901</td>\n",
       "      <td>2.420304</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-15</td>\n",
       "      <td>4.308812</td>\n",
       "      <td>-0.386148</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-13</td>\n",
       "      <td>-6.739469</td>\n",
       "      <td>-8.225806</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2</td>\n",
       "      <td>-3.494952</td>\n",
       "      <td>-0.689150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>16</td>\n",
       "      <td>7.190361</td>\n",
       "      <td>1.561670</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>4</td>\n",
       "      <td>3.718224</td>\n",
       "      <td>-0.439338</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>4</td>\n",
       "      <td>7.235661</td>\n",
       "      <td>4.716912</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>-31</td>\n",
       "      <td>4.168459</td>\n",
       "      <td>5.968750</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>-5</td>\n",
       "      <td>3.635816</td>\n",
       "      <td>-0.931985</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.992145</td>\n",
       "      <td>-2.709447</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.006471</td>\n",
       "      <td>-7.785282</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.217434</td>\n",
       "      <td>-6.141098</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>-12</td>\n",
       "      <td>-3.079451</td>\n",
       "      <td>-5.891544</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>-4</td>\n",
       "      <td>-0.184001</td>\n",
       "      <td>4.474383</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>17</td>\n",
       "      <td>4.266836</td>\n",
       "      <td>4.294118</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>14</td>\n",
       "      <td>5.154762</td>\n",
       "      <td>-2.248577</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>1</td>\n",
       "      <td>6.200486</td>\n",
       "      <td>-1.946524</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>10</td>\n",
       "      <td>5.682517</td>\n",
       "      <td>-1.636364</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>13</td>\n",
       "      <td>-6.769730</td>\n",
       "      <td>-0.342246</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>-26</td>\n",
       "      <td>-12.506552</td>\n",
       "      <td>-4.050710</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>-12</td>\n",
       "      <td>-8.571692</td>\n",
       "      <td>-5.165775</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>-26</td>\n",
       "      <td>-12.779236</td>\n",
       "      <td>-6.685662</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>-23</td>\n",
       "      <td>-12.037906</td>\n",
       "      <td>-6.323529</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>4</td>\n",
       "      <td>3.891169</td>\n",
       "      <td>5.619960</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>17</td>\n",
       "      <td>0.770274</td>\n",
       "      <td>1.172014</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>15</td>\n",
       "      <td>9.958699</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>11</td>\n",
       "      <td>1.176538</td>\n",
       "      <td>-5.954167</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>-10</td>\n",
       "      <td>6.652279</td>\n",
       "      <td>6.701857</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>-7</td>\n",
       "      <td>1.161747</td>\n",
       "      <td>3.572825</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>6</td>\n",
       "      <td>-4.024691</td>\n",
       "      <td>1.219697</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>4</td>\n",
       "      <td>7.132984</td>\n",
       "      <td>-1.949198</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>4</td>\n",
       "      <td>5.370500</td>\n",
       "      <td>1.591800</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>2</td>\n",
       "      <td>0.072980</td>\n",
       "      <td>2.470588</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>-1</td>\n",
       "      <td>6.436714</td>\n",
       "      <td>9.126838</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>-16</td>\n",
       "      <td>-1.266424</td>\n",
       "      <td>3.535038</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>18</td>\n",
       "      <td>15.116488</td>\n",
       "      <td>4.631527</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>5</td>\n",
       "      <td>-6.255080</td>\n",
       "      <td>-5.732008</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>4</td>\n",
       "      <td>2.510790</td>\n",
       "      <td>2.568015</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>20</td>\n",
       "      <td>-7.205825</td>\n",
       "      <td>4.269795</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>-17</td>\n",
       "      <td>-2.677187</td>\n",
       "      <td>-7.655172</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>12</td>\n",
       "      <td>3.657790</td>\n",
       "      <td>-2.470588</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>19</td>\n",
       "      <td>3.765709</td>\n",
       "      <td>0.348485</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>-3</td>\n",
       "      <td>2.513477</td>\n",
       "      <td>4.167614</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>5</td>\n",
       "      <td>-1.568620</td>\n",
       "      <td>2.861193</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>3</td>\n",
       "      <td>7.799290</td>\n",
       "      <td>-5.142602</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>23</td>\n",
       "      <td>-0.239277</td>\n",
       "      <td>2.030303</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>16</td>\n",
       "      <td>2.167216</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      real  predicted    FGAdif  correct\n",
       "981    -10  -2.823683  3.519886     True\n",
       "982      7  -3.436291 -3.531250    False\n",
       "983    -15  -3.480578 -2.687500     True\n",
       "984     -4   2.259196  5.833822    False\n",
       "985     -2   4.966380  2.303030    False\n",
       "986     18   4.172597  1.596774     True\n",
       "987     -3   6.553689  1.387868    False\n",
       "988    -21   3.257656 -7.758467    False\n",
       "989     20   0.333798 -7.199643     True\n",
       "990    -22  -8.000342 -0.818182     True\n",
       "991      4  -3.250805 -1.409982    False\n",
       "992     -4  -9.588147 -7.515152     True\n",
       "993    -25  -7.330038 -0.814394     True\n",
       "994      4   8.564901  2.420304     True\n",
       "995    -15   4.308812 -0.386148    False\n",
       "996    -13  -6.739469 -8.225806     True\n",
       "997      2  -3.494952 -0.689150    False\n",
       "998     16   7.190361  1.561670     True\n",
       "999      4   3.718224 -0.439338     True\n",
       "1000     4   7.235661  4.716912     True\n",
       "1001   -31   4.168459  5.968750    False\n",
       "1002    -5   3.635816 -0.931985    False\n",
       "1003     3  -0.992145 -2.709447    False\n",
       "1004     2  -1.006471 -7.785282    False\n",
       "1005     1  -0.217434 -6.141098    False\n",
       "1006   -12  -3.079451 -5.891544     True\n",
       "1007    -4  -0.184001  4.474383     True\n",
       "1008    17   4.266836  4.294118     True\n",
       "1009    14   5.154762 -2.248577     True\n",
       "1010     1   6.200486 -1.946524     True\n",
       "...    ...        ...       ...      ...\n",
       "1018    10   5.682517 -1.636364     True\n",
       "1019    13  -6.769730 -0.342246    False\n",
       "1020   -26 -12.506552 -4.050710     True\n",
       "1021   -12  -8.571692 -5.165775     True\n",
       "1022   -26 -12.779236 -6.685662     True\n",
       "1023   -23 -12.037906 -6.323529     True\n",
       "1024     4   3.891169  5.619960     True\n",
       "1025    17   0.770274  1.172014     True\n",
       "1026    15   9.958699  0.625000     True\n",
       "1027    11   1.176538 -5.954167     True\n",
       "1028   -10   6.652279  6.701857    False\n",
       "1029    -7   1.161747  3.572825    False\n",
       "1030     6  -4.024691  1.219697    False\n",
       "1031     4   7.132984 -1.949198     True\n",
       "1032     4   5.370500  1.591800     True\n",
       "1033     2   0.072980  2.470588     True\n",
       "1034    -1   6.436714  9.126838    False\n",
       "1035   -16  -1.266424  3.535038     True\n",
       "1036    18  15.116488  4.631527     True\n",
       "1037     5  -6.255080 -5.732008    False\n",
       "1038     4   2.510790  2.568015     True\n",
       "1039    20  -7.205825  4.269795    False\n",
       "1040   -17  -2.677187 -7.655172     True\n",
       "1041    12   3.657790 -2.470588     True\n",
       "1042    19   3.765709  0.348485     True\n",
       "1043    -3   2.513477  4.167614    False\n",
       "1044     5  -1.568620  2.861193    False\n",
       "1045     3   7.799290 -5.142602     True\n",
       "1046    23  -0.239277  2.030303    False\n",
       "1047    16   2.167216  1.000000     True\n",
       "\n",
       "[67 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6119402985074627"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "results['real'] = test_year.score_dif\n",
    "results['predicted'] = grid.predict(test_year.drop(columns=['score_dif', 'Season']))\n",
    "results['FGAdif'] = test_year.FGA\n",
    "\n",
    "results[results['real'] * results['predicted'] > 0]\n",
    "results['correct'] = results['real'] * results['predicted'] > 0\n",
    "display(results)\n",
    "len(results[results['real'] * results['predicted'] > 0]) / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Season  WTeamID  LTeamID  score_dif  TeamID       FGM        FGA  \\\n",
      "0       2003     1421     1411          8    1421 -0.354023   1.526437   \n",
      "1       2003     1421     1400        -21    1421 -3.620690  -5.635468   \n",
      "2       2003     1277     1400         -9    1277 -4.967742 -11.331797   \n",
      "3       2003     1345     1400        -10    1345 -4.250000  -8.035714   \n",
      "4       2003     1163     1400         -4    1163  1.533333  -0.228571   \n",
      "5       2003     1436     1112        -29    1436 -5.493842  -9.852217   \n",
      "6       2003     1242     1112          3    1242 -0.088095  -3.414286   \n",
      "7       2003     1323     1112        -17    1323 -3.095622  -5.036866   \n",
      "8       2003     1272     1113        -13    1272 -0.931034   3.103448   \n",
      "9       2003     1242     1113         32    1242  3.026437   5.403448   \n",
      "10      2003     1141     1166          6    1141 -2.076280  -4.764890   \n",
      "11      2003     1143     1301          2    1143  3.011494   5.390805   \n",
      "12      2003     1143     1328         -9    1143  2.078161   2.190805   \n",
      "13      2003     1140     1163         -5    1140 -5.501075 -10.941935   \n",
      "14      2003     1390     1163        -11    1390 -3.888172  -4.167742   \n",
      "15      2003     1181     1161         10    1181  0.966667   8.233333   \n",
      "16      2003     1181     1141         26    1181  0.745977   7.643678   \n",
      "17      2003     1181     1242         -4    1181 -2.866667  -1.966667   \n",
      "18      2003     1153     1211         -5    1153 -3.171659   1.226959   \n",
      "19      2003     1112     1211          1    1112  4.256912  10.262673   \n",
      "20      2003     1228     1443          5    1228  1.390323  -0.103226   \n",
      "21      2003     1242     1429          3    1242  6.533333  10.366667   \n",
      "22      2003     1242     1266         33    1242  3.019048   6.050000   \n",
      "23      2003     1281     1266         -9    1281 -1.052995   2.943548   \n",
      "24      2003     1246     1266        -14    1246  1.254464   2.156250   \n",
      "25      2003     1242     1393         -3    1242  0.991954   0.093103   \n",
      "26      2003     1328     1393        -16    1328 -3.974713  -5.673563   \n",
      "27      2003     1329     1393        -12    1329 -4.586207  -6.793103   \n",
      "28      2003     1400     1393        -11    1400 -1.241379   0.221675   \n",
      "29      2003     1266     1221          4    1266  3.007389   3.525862   \n",
      "...      ...      ...      ...        ...     ...       ...        ...   \n",
      "1018    2018     1403     1372         10    1403 -1.281818  -1.636364   \n",
      "1019    2018     1403     1345         13    1403 -2.024955  -0.342246   \n",
      "1020    2018     1168     1345        -26    1168 -3.348884  -4.050710   \n",
      "1021    2018     1403     1437        -12    1403 -4.877897  -5.165775   \n",
      "1022    2018     1347     1437        -26    1347 -7.748162  -6.685662   \n",
      "1023    2018     1104     1437        -23    1104 -5.735294  -6.323529   \n",
      "1024    2018     1120     1158          4    1120  1.254032   5.619960   \n",
      "1025    2018     1139     1116         17    1139  0.241533   1.172014   \n",
      "1026    2018     1153     1209         15    1153 -0.141544   0.625000   \n",
      "1027    2018     1155     1308         11    1155 -2.918750  -5.954167   \n",
      "1028    2018     1166     1243        -10    1166  4.669599   6.701857   \n",
      "1029    2018     1420     1243         -7    1420 -0.265885   3.572825   \n",
      "1030    2018     1267     1455          6    1267 -0.092803   1.219697   \n",
      "1031    2018     1277     1137          4    1277  0.869875  -1.949198   \n",
      "1032    2018     1305     1400          4    1305  2.535651   1.591800   \n",
      "1033    2018     1305     1153          2    1305  2.058824   2.470588   \n",
      "1034    2018     1305     1260         -1    1305  2.323529   9.126838   \n",
      "1035    2018     1243     1260        -16    1243 -0.121212   3.535038   \n",
      "1036    2018     1314     1252         18    1314  2.425616   4.631527   \n",
      "1037    2018     1393     1395          5    1393 -7.242424  -5.732008   \n",
      "1038    2018     1401     1344          4    1401  2.071691   2.568015   \n",
      "1039    2018     1420     1438         20    1420  0.612903   4.269795   \n",
      "1040    2018     1293     1452        -17    1293 -0.868154  -7.655172   \n",
      "1041    2018     1437     1452         12    1437  3.264706  -2.470588   \n",
      "1042    2018     1462     1411         19    1462  3.055258   0.348485   \n",
      "1043    2018     1196     1403         -3    1196  0.129735   4.167614   \n",
      "1044    2018     1199     1462          5    1199  0.379277   2.861193   \n",
      "1045    2018     1345     1139          3    1345 -1.035651  -5.142602   \n",
      "1046    2018     1452     1267         23    1452 -1.204991   2.030303   \n",
      "1047    2018     1437     1242         16    1437  0.911765   1.000000   \n",
      "\n",
      "          FGM3      FGA3       FTM       FTA        OR        DR       Ast  \\\n",
      "0     0.549425 -0.500000 -1.434483 -7.135632 -0.890805 -1.627586 -1.165517   \n",
      "1     0.625616  1.214286 -1.034483 -2.854680 -3.902709 -2.970443 -1.465517   \n",
      "2    -0.953917 -3.398618 -0.741935 -1.656682 -5.501152 -1.884793 -0.887097   \n",
      "3    -0.750000 -1.285714  2.142857  1.714286 -5.785714 -2.678571 -2.250000   \n",
      "4     0.209524 -1.085714 -2.100000 -1.685714 -1.411905  1.757143  1.133333   \n",
      "5    -1.759852 -4.588670 -4.673645 -5.448276 -2.213054 -1.918719 -3.435961   \n",
      "6    -2.235714 -5.938095 -1.469048 -0.866667 -0.878571 -0.742857 -0.909524   \n",
      "7     1.254608  1.702765 -0.180876 -2.161290 -3.791475 -0.771889 -0.739631   \n",
      "8     3.000000  7.482759 -2.586207 -3.310345  0.379310  2.655172  1.068966   \n",
      "9     0.800000  1.547126 -1.485057 -2.073563  0.610345  3.589655  1.181609   \n",
      "10   -1.142111 -2.553814  5.397074  5.142111 -0.292581  0.094044 -1.197492   \n",
      "11   -1.552874 -5.465517 -2.387356 -0.949425  1.508046  2.345977  1.333333   \n",
      "12   -1.052874 -1.932184  0.212644  0.917241 -0.891954 -0.587356  1.833333   \n",
      "13    0.126882  0.429032  3.293548  2.061290 -3.895699 -3.480645 -2.213978   \n",
      "14    0.868817  3.687097 -0.867742 -1.325806 -1.024731 -2.770968 -0.988172   \n",
      "15    3.133333  9.033333  2.900000  3.800000  2.966667 -0.366667 -1.666667   \n",
      "16    0.505747  2.668966  0.624138  2.894253  3.180460 -0.175862 -1.787356   \n",
      "17    2.533333  6.466667  3.833333  3.933333 -0.533333 -3.800000 -2.900000   \n",
      "18   -0.482719  0.435484 -2.917051 -3.109447  0.207373 -1.929724 -3.456221   \n",
      "19   -0.125576  1.006912 -0.238479  0.354839  3.243088  2.320276  1.900922   \n",
      "20   -0.276344 -0.865591 -0.167742 -1.996774 -2.427957  2.141935  4.073118   \n",
      "21    0.766667  2.000000  1.533333  3.900000  1.666667  4.533333  2.833333   \n",
      "22   -0.985714 -1.116667 -2.111905  0.526190  1.192857  2.828571  0.411905   \n",
      "23    1.698157  5.717742 -6.210829 -5.445853  0.376728  1.831797 -2.579493   \n",
      "24    0.120536  1.343750 -3.491071 -2.825893 -0.232143  0.084821 -0.133929   \n",
      "25   -0.441379 -1.728736 -0.312644  0.512644 -0.010345  0.003448  1.767816   \n",
      "26    2.225287  3.104598 -3.212644 -5.020690 -2.177011 -1.929885 -0.798851   \n",
      "27   -0.379310 -2.896552 -1.827586 -1.275862 -1.931034 -2.827586 -2.724138   \n",
      "28    0.615764  0.923645  0.620690  0.165025  1.868227 -0.753695 -0.465517   \n",
      "29    0.475369  0.491379  2.557882  1.469212  1.038177 -0.238916  0.907635   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1018 -0.003030  0.412121 -0.357576 -0.503030 -0.266667  1.172727  0.857576   \n",
      "1019 -2.557932 -3.611408  0.703209  2.226381  2.558824 -1.648841 -2.281640   \n",
      "1020 -4.450304 -7.858012  2.656187  3.770791 -0.130832 -1.002028 -4.326572   \n",
      "1021 -4.381462 -9.464349  2.320856  5.079323  1.617647 -1.266488 -2.252228   \n",
      "1022 -3.755515 -6.863971 -0.869485 -0.273897  1.430147 -3.205882 -4.801471   \n",
      "1023 -5.029412 -9.000000  1.794118  5.264706  0.647059  0.058824 -4.147059   \n",
      "1024  2.169355  5.786290  3.504032  3.978831  3.770161  1.579637  3.372984   \n",
      "1025  0.387701  3.558824 -2.837790 -6.404635 -0.964349  0.199643 -0.470588   \n",
      "1026 -1.924632 -3.152574  2.272059  3.198529  4.402574  1.009191  3.158088   \n",
      "1027  0.679167  0.220833  2.839583  1.031250 -3.631250 -1.900000 -0.579167   \n",
      "1028  3.415445  7.334311 -1.150538 -1.684262 -0.988270  6.315738  3.505376   \n",
      "1029  2.899316  5.592375 -2.247312 -0.813294  1.527859  1.960899  0.537634   \n",
      "1030  0.935606  4.585227 -0.143939 -0.980114 -4.125947 -0.616477 -1.561553   \n",
      "1031  0.482175 -2.725490 -2.399287 -4.278075  1.707665  2.287879  4.862745   \n",
      "1032  2.614973  1.926916  3.674688  2.866310 -1.105169  1.346702  4.865419   \n",
      "1033  2.411765  3.970588  2.088235  1.088235 -3.588235  0.705882  0.470588   \n",
      "1034  2.362132  6.068015  4.391544  5.380515  3.314338  0.840074  1.003676   \n",
      "1035 -0.404356  1.838068  1.322917  1.362689  2.025568 -3.324811 -1.104167   \n",
      "1036  0.527094 -0.774384 -4.268966 -6.302463  3.919212  0.911330  3.297537   \n",
      "1037 -2.620265 -2.974432  1.650568  1.317235  0.840909  0.145833 -7.811553   \n",
      "1038  0.211397  0.977941 -3.060662 -3.356618  1.856618  3.575368  1.704044   \n",
      "1039  2.475073  6.895406  1.237537  4.156403  1.315738  0.082111  1.143695   \n",
      "1040 -0.414807 -2.532454 -0.087221  0.925963 -3.482759  2.393509 -0.713996   \n",
      "1041  2.617647  3.764706 -1.705882 -2.352941 -4.617647  2.323529  1.617647   \n",
      "1042 -0.005348 -0.347594  0.520499 -1.627451 -0.506239  2.723708  4.371658   \n",
      "1043  2.094697  5.100379 -1.534091 -2.696970 -0.218750 -0.595644 -1.455492   \n",
      "1044  0.181818  1.592375 -3.438905 -1.376344  1.921799 -2.283480 -1.378299   \n",
      "1045  1.406417 -0.176471  2.690731  4.228164 -0.770945  1.800357  2.705882   \n",
      "1046 -1.266488 -3.360963 -0.311943 -0.393048  5.969697 -3.844920 -1.910873   \n",
      "1047  1.264706  3.470588  2.470588  1.735294 -0.176471  0.558824 -0.323529   \n",
      "\n",
      "            TO       Stl       Blk        PF  \n",
      "0     0.973563  0.635632  0.766667  0.803448  \n",
      "1     2.778325  0.676108 -0.857143 -1.253695  \n",
      "2     1.119816 -0.005760 -0.147465 -0.324885  \n",
      "3     0.392857  0.785714 -0.928571 -0.535714  \n",
      "4     2.371429 -0.459524  3.876190 -1.957143  \n",
      "5    -0.716749 -1.602217 -1.248768 -1.853448  \n",
      "6     0.114286  1.669048  0.685714 -1.050000  \n",
      "7    -2.011521 -1.012673  1.430876 -1.524194  \n",
      "8    -0.206897  2.172414  0.827586 -0.655172  \n",
      "9     0.900000  4.926437  0.658621 -2.713793  \n",
      "10    4.877743 -1.290491 -0.454545  3.692790  \n",
      "11   -0.027586 -1.214943 -0.273563 -1.563218  \n",
      "12    2.372414 -0.381609 -0.973563 -1.496552  \n",
      "13   -2.058065  1.002151 -5.217204  3.019355  \n",
      "14   -1.477419 -0.417204 -4.378495  0.116129  \n",
      "15   -2.100000  3.166667  0.900000  0.700000  \n",
      "16   -4.208046  1.396552  1.133333  0.301149  \n",
      "17   -0.866667 -1.633333  0.233333  4.566667  \n",
      "18   -3.941244 -1.627880  0.733871  0.319124  \n",
      "19    0.237327  1.657834  0.698157 -0.895161  \n",
      "20   -1.994624 -0.064516 -0.448387  0.681720  \n",
      "21    2.666667  5.066667  2.900000  0.600000  \n",
      "22    1.328571  4.133333  1.257143 -1.942857  \n",
      "23    0.041475 -0.290323  0.679724 -1.771889  \n",
      "24    0.491071  1.812500  1.450893 -1.267857  \n",
      "25    1.279310  1.822989 -2.375862  0.113793  \n",
      "26   -1.820690 -1.377011 -3.509195  2.013793  \n",
      "27    1.034483  0.482759 -1.689655  3.551724  \n",
      "28   -0.192118 -1.917488 -3.418719  3.770936  \n",
      "29   -0.359606 -1.965517 -0.839901  0.263547  \n",
      "...        ...       ...       ...       ...  \n",
      "1018 -2.621212 -2.106061  1.306061 -3.681818  \n",
      "1019  1.868984  1.570410 -0.756684  2.622103  \n",
      "1020  3.806288  0.590264 -1.684584  2.711968  \n",
      "1021  2.192513  0.717469  0.302139  2.298574  \n",
      "1022  1.647059 -0.363971 -1.064338  0.897059  \n",
      "1023  3.794118 -0.264706  1.500000  3.147059  \n",
      "1024  2.379032  2.082661  1.728831  3.229839  \n",
      "1025  0.181818  0.491979 -1.678253 -1.963458  \n",
      "1026  0.001838  0.137868  1.060662 -2.738971  \n",
      "1027 -0.172917  0.154167  1.843750 -2.166667  \n",
      "1028 -0.099707 -2.304008 -0.548387 -3.071359  \n",
      "1029  0.222874 -0.400782 -0.451613 -1.877810  \n",
      "1030  1.589015  1.888258  2.001894 -0.230114  \n",
      "1031  0.564171 -1.526738  2.922460 -0.527629  \n",
      "1032 -1.907308  0.052585 -0.881462  1.143494  \n",
      "1033 -1.529412 -1.029412 -1.441176  1.441176  \n",
      "1034 -2.433824 -0.391544  1.713235  3.264706  \n",
      "1035 -0.920455  1.131629  0.625000  4.393939  \n",
      "1036 -3.592118 -0.653202  1.360591 -4.561576  \n",
      "1037 -0.111742  0.648674  1.982008 -0.603220  \n",
      "1038  1.091912 -1.259191  2.141544 -0.926471  \n",
      "1039  3.101662  0.599218 -1.178886  2.455523  \n",
      "1040  0.400609 -1.443205 -2.230223 -3.968560  \n",
      "1041 -0.970588 -1.352941 -1.294118 -5.529412  \n",
      "1042 -0.193405  0.260250 -1.001783 -0.795900  \n",
      "1043 -2.732955 -0.487689  0.446023 -0.901515  \n",
      "1044  0.575758  0.861193  2.221896  1.350929  \n",
      "1045 -0.505348 -0.903743  2.090018 -2.713012  \n",
      "1046 -1.827986  1.453654 -0.674688  4.018717  \n",
      "1047 -1.323529  0.058824 -0.205882  0.558824  \n",
      "\n",
      "[1048 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:426: FutureWarning: You should specify a value for 'n_splits' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "resultsWin = pd.read_csv('data/RegularSeasonDetailedResults.csv')\n",
    "teams = pd.read_csv('data/Teams.csv')\n",
    "\n",
    "# Get only winning teams game stats to predict their score\n",
    "resultsWin = resultsWin.drop(['WTeamID', 'LTeamID', 'WLoc'], axis=1)\n",
    "\n",
    "def neuralNetwork(results) :\n",
    "    train_features, test_features, train_outcome, test_outcome = train_test_split(\n",
    "        results.drop(\"WScore\", axis=1),\n",
    "        results.WScore,\n",
    "        test_size=0.30, \n",
    "        random_state=11\n",
    "    )\n",
    "    scaler = MinMaxScaler()\n",
    "    mlp_reg = MLPClassifier()\n",
    "\n",
    "    imputer = Imputer()\n",
    "    selector = SelectPercentile()\n",
    "    threshold = VarianceThreshold(.1)\n",
    "    pipe = make_pipeline(imputer, threshold, selector, scaler, mlp_reg)\n",
    "\n",
    "    param_grid = {\n",
    "        'selectpercentile__percentile':range(10, 30, 5)\n",
    "        }\n",
    "\n",
    "    crossVal = KFold()\n",
    "    grid = GridSearchCV(pipe, param_grid, cv = crossVal, scoring=\"neg_mean_absolute_error\")\n",
    "    grid.fit(train_features, train_outcome)\n",
    "    grid.score(test_features, test_outcome)\n",
    "\n",
    "    score = grid.score(test_features, test_outcome)\n",
    "\n",
    "    predictedValues = grid.predict(test_features)\n",
    "\n",
    "    return [score, predictedValues, grid, test_outcome]\n",
    "\n",
    "neural = neuralNetwork(resultsWin)\n",
    "\n",
    "# How many points off were we from predicting the winning score?\n",
    "print(neural[0])\n",
    "\n",
    "# This can be exactly replicated for the other team using LScore for losing score\n",
    "# Then need to randomize which teams get which grid and we run the grid for each playoff game invididually\n",
    "# in their own csv. Take the two scores, see who won, move them manually to the next round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsWin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not quite done withe the following but it is a way to predict the outcomes of the tournament games as they contunue. The games each round can be based off of the predicted outcome of the last round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_round(teams_df):\n",
    "    '''Takes a dataframe with two columns: School id, and ranking, none of \n",
    "    these teams should have been eliminated. It returns the next matchups for these \n",
    "    teams based on their ranking'''\n",
    "    arar = np.char.array(['01','16','08','09','05','12','04','13','06','11','03','14','07','10','02','15'])\n",
    "    arr = np.append(arar, arar)\n",
    "    first_round_bracket = np.char.array(['W', 'X', 'Y', 'Z']).repeat(16) + np.append(arr, arr)\n",
    "    if(len(teams_df) < 64):\n",
    "        won_ranks = teams_df['Seed'].values\n",
    "        first_round_bracket = np.array([x for x in first_round_bracket if x in won_ranks])\n",
    "    #print(first_round_bracket)\n",
    "    picks1 = first_round_bracket[np.arange(1, len(teams_df), 2)]\n",
    "    picks2 = first_round_bracket[np.arange(0, len(teams_df), 2)]\n",
    "    #print(picks1)\n",
    "    #print(picks2)\n",
    "    teams_df = teams_df.set_index('Seed')\n",
    "    teams_next = pd.DataFrame()\n",
    "    teams_next['team1'] = teams_df.loc[picks1, 'TeamID'].values\n",
    "    teams_next['team2'] = teams_df.loc[picks2, 'TeamID'].values\n",
    "    teams_next['rank1'] = picks1\n",
    "    teams_next['rank2'] = picks2\n",
    "    return teams_next\n",
    "\n",
    "def find_winners(nx):\n",
    "    '''Takes a dataframe with teamsids and ranks from matches and keeps only the \n",
    "    winners'''\n",
    "    nx['score'] = (nx['team1'] - nx['team2'])\n",
    "    nx['TeamID'] = nx.loc[:, 'team2']\n",
    "    nx['Seed'] = nx.loc[:, 'rank2']\n",
    "    \n",
    "    condition = nx['score'] > 0\n",
    "    #this can be replaced with an ouput from a neural net to predict winners\n",
    "    underdogs = nx.loc[condition,['rank1', 'team1']]\n",
    "    underdogs.columns = ['Seed', 'TeamID']\n",
    "    nx.update(underdogs)\n",
    "    return(nx)#.loc[:, ['Seed', 'TeamID']])\n",
    "\n",
    "def first_four(teams_df, games_record):\n",
    "    pregames = teams_df.loc[teams_df['Seed'].str.contains('a|b'),:]\n",
    "    teams_df = teams_df.loc[~teams_df['Seed'].str.contains('a|b'),:]\n",
    "    #features = pd.merge(games, team_summary_stats, how='left', left_on=['team1'], right_on=['TeamID'])\n",
    "    #features = pd.merge(games, team_summary_stats, how='left', left_on=['team2'], right_on=['TeamID'], suffixes=('', '_t2'))\n",
    "    teams_next = pd.DataFrame()\n",
    "    teams_next['team1'] = pregames.iloc[np.arange(1, len(pregames), 2), 1].values\n",
    "    teams_next['team2'] = pregames.iloc[np.arange(0, len(pregames), 2), 1].values\n",
    "    teams_next['rank1'] = pregames.iloc[np.arange(1, len(pregames), 2), 0].values\n",
    "    teams_next['rank2'] = pregames.iloc[np.arange(0, len(pregames), 2), 0].values\n",
    "    to_begin = find_winners(teams_next.copy())\n",
    "    to_begin['round'] = 0\n",
    "    if len(games_record) < 1:\n",
    "        games_record = to_begin.copy()#.loc[:,:]\n",
    "    else:\n",
    "        games_record = games_record.append(to_begin.copy(), ignore_index=True)\n",
    "    to_begin['Seed'] = [x[0:-1] for x in to_begin['Seed'].values]\n",
    "    teams_df = teams_df.append(to_begin.loc[:, ['Seed', 'TeamID']])\n",
    "    #print(to_begin.loc[:, ['Seed', 'TeamID']])\n",
    "    return([teams_df, games_record])\n",
    "\n",
    "games_record = pd.DataFrame() # records all matches\n",
    "teams_df = pd.read_csv('data/NCAATourneySeeds.csv')\n",
    "teams_df = teams_df.loc[teams_df.Season == 2003, ['Seed', 'TeamID']]\n",
    "teams_df, games_record = first_four(teams_df, games_record)\n",
    "#print(teams_df)\n",
    "round = 1 \n",
    "\n",
    "#runs until only 1 team remains\n",
    "while len(teams_df) > 1:\n",
    "    games = next_round(teams_df)\n",
    "    features = pd.merge(games, team_summary_stats, how='left', left_on=['team1'], right_on=['TeamID'])\n",
    "    features = pd.merge(features, team_summary_stats, how='left', left_on=['team2'], right_on=['TeamID'], suffixes=('', '_t2'))\n",
    "    print(features.head(2))\n",
    "    games['round'] = round\n",
    "    teams_df = find_winners(games)\n",
    "    if len(games_record) < 1:\n",
    "        games_record = games#.loc[:,:]\n",
    "    else:\n",
    "        games_record = games_record.append(games, ignore_index=True)\n",
    "    round = round + 1\n",
    "#print(games_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_season_results = pd.read_csv('data/RegularSeasonDetailedResults.csv')\n",
    "post_season_outcomes = pd.read_csv('data/NCAATourneyDetailedResults.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a set of regular season features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Season  TeamID        FGM        FGA      FGM3       FGA3        FTM  \\\n",
      "0    2003    1102  19.142857  39.785714  7.821429  20.821429  11.142857   \n",
      "1    2003    1103  27.148148  55.851852  5.444444  16.074074  19.037037   \n",
      "2    2003    1104  24.035714  57.178571  6.357143  19.857143  14.857143   \n",
      "3    2003    1105  24.384615  61.615385  7.576923  20.769231  15.423077   \n",
      "4    2003    1106  23.428571  55.285714  6.107143  17.642857  10.642857   \n",
      "\n",
      "         FTA         OR         DR        Ast         TO       Stl       Blk  \\\n",
      "0  17.107143   4.178571  16.821429  13.000000  11.428571  5.964286  1.785714   \n",
      "1  25.851852   9.777778  19.925926  15.222222  12.629630  7.259259  2.333333   \n",
      "2  20.928571  13.571429  23.928571  12.107143  13.285714  6.607143  3.785714   \n",
      "3  21.846154  13.500000  23.115385  14.538462  18.653846  9.307692  2.076923   \n",
      "4  16.464286  12.285714  23.857143  11.678571  17.035714  8.357143  3.142857   \n",
      "\n",
      "          PF  \n",
      "0  18.750000  \n",
      "1  19.851852  \n",
      "2  18.035714  \n",
      "3  20.230769  \n",
      "4  18.178571  \n"
     ]
    }
   ],
   "source": [
    "winners = regular_season_results.loc[:,['Season', 'WTeamID', 'WFGM', 'WFGA', 'WFGM3', 'WFGA3', \n",
    "                                         'WFTM', 'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF']]\n",
    "losers = regular_season_results.loc[:,['Season', 'LTeamID', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3',\n",
    "                                      'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']]\n",
    "winners.columns = ['Season', 'TeamID', 'FGM', 'FGA', 'FGM3', 'FGA3',\n",
    "                                      'FTM', 'FTA', 'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF']\n",
    "losers.columns = ['Season', 'TeamID', 'FGM', 'FGA', 'FGM3', 'FGA3',\n",
    "                                      'FTM', 'FTA', 'OR', 'DR', 'Ast', 'TO', 'Stl', 'Blk', 'PF']\n",
    "all_teams = winners.copy()\n",
    "all_teams = all_teams.append(losers.copy(), ignore_index=True)\n",
    "team_summary_stats = all_teams.groupby(['Season', 'TeamID'], as_index=False).mean()\n",
    "print(team_summary_stats.head())\n",
    "team_summary_stats.to_csv('data/team_summary_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a set of post season outcomes to test with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_season_outcomes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Season  WTeamID  LTeamID  score_dif\n",
      "0    2003     1421     1411          8\n",
      "1    2003     1112     1436         29\n",
      "2    2003     1113     1272         13\n",
      "3    2003     1141     1166          6\n",
      "4    2003     1143     1301          2\n",
      "   Season  WTeamID  LTeamID  score_dif\n",
      "0    2003     1421     1411          8\n",
      "1    2003     1436     1112        -29\n",
      "2    2003     1272     1113        -13\n",
      "3    2003     1141     1166          6\n",
      "4    2003     1143     1301          2\n",
      "1048\n",
      "1048\n",
      "1048\n",
      "    Season  WTeamID  LTeamID  score_dif  TeamID       FGM        FGA  \\\n",
      "0     2003     1421     1411          8    1421 -0.354023   1.526437   \n",
      "1     2003     1421     1400        -21    1421 -3.620690  -5.635468   \n",
      "2     2003     1277     1400         -9    1277 -4.967742 -11.331797   \n",
      "3     2003     1345     1400        -10    1345 -4.250000  -8.035714   \n",
      "4     2003     1163     1400         -4    1163  1.533333  -0.228571   \n",
      "5     2003     1436     1112        -29    1436 -5.493842  -9.852217   \n",
      "6     2003     1242     1112          3    1242 -0.088095  -3.414286   \n",
      "7     2003     1323     1112        -17    1323 -3.095622  -5.036866   \n",
      "8     2003     1272     1113        -13    1272 -0.931034   3.103448   \n",
      "9     2003     1242     1113         32    1242  3.026437   5.403448   \n",
      "10    2003     1141     1166          6    1141 -2.076280  -4.764890   \n",
      "11    2003     1143     1301          2    1143  3.011494   5.390805   \n",
      "12    2003     1143     1328         -9    1143  2.078161   2.190805   \n",
      "13    2003     1140     1163         -5    1140 -5.501075 -10.941935   \n",
      "14    2003     1390     1163        -11    1390 -3.888172  -4.167742   \n",
      "\n",
      "        FGM3      FGA3       FTM       FTA        OR        DR       Ast  \\\n",
      "0   0.549425 -0.500000 -1.434483 -7.135632 -0.890805 -1.627586 -1.165517   \n",
      "1   0.625616  1.214286 -1.034483 -2.854680 -3.902709 -2.970443 -1.465517   \n",
      "2  -0.953917 -3.398618 -0.741935 -1.656682 -5.501152 -1.884793 -0.887097   \n",
      "3  -0.750000 -1.285714  2.142857  1.714286 -5.785714 -2.678571 -2.250000   \n",
      "4   0.209524 -1.085714 -2.100000 -1.685714 -1.411905  1.757143  1.133333   \n",
      "5  -1.759852 -4.588670 -4.673645 -5.448276 -2.213054 -1.918719 -3.435961   \n",
      "6  -2.235714 -5.938095 -1.469048 -0.866667 -0.878571 -0.742857 -0.909524   \n",
      "7   1.254608  1.702765 -0.180876 -2.161290 -3.791475 -0.771889 -0.739631   \n",
      "8   3.000000  7.482759 -2.586207 -3.310345  0.379310  2.655172  1.068966   \n",
      "9   0.800000  1.547126 -1.485057 -2.073563  0.610345  3.589655  1.181609   \n",
      "10 -1.142111 -2.553814  5.397074  5.142111 -0.292581  0.094044 -1.197492   \n",
      "11 -1.552874 -5.465517 -2.387356 -0.949425  1.508046  2.345977  1.333333   \n",
      "12 -1.052874 -1.932184  0.212644  0.917241 -0.891954 -0.587356  1.833333   \n",
      "13  0.126882  0.429032  3.293548  2.061290 -3.895699 -3.480645 -2.213978   \n",
      "14  0.868817  3.687097 -0.867742 -1.325806 -1.024731 -2.770968 -0.988172   \n",
      "\n",
      "          TO       Stl       Blk        PF  \n",
      "0   0.973563  0.635632  0.766667  0.803448  \n",
      "1   2.778325  0.676108 -0.857143 -1.253695  \n",
      "2   1.119816 -0.005760 -0.147465 -0.324885  \n",
      "3   0.392857  0.785714 -0.928571 -0.535714  \n",
      "4   2.371429 -0.459524  3.876190 -1.957143  \n",
      "5  -0.716749 -1.602217 -1.248768 -1.853448  \n",
      "6   0.114286  1.669048  0.685714 -1.050000  \n",
      "7  -2.011521 -1.012673  1.430876 -1.524194  \n",
      "8  -0.206897  2.172414  0.827586 -0.655172  \n",
      "9   0.900000  4.926437  0.658621 -2.713793  \n",
      "10  4.877743 -1.290491 -0.454545  3.692790  \n",
      "11 -0.027586 -1.214943 -0.273563 -1.563218  \n",
      "12  2.372414 -0.381609 -0.973563 -1.496552  \n",
      "13 -2.058065  1.002151 -5.217204  3.019355  \n",
      "14 -1.477419 -0.417204 -4.378495  0.116129  \n",
      "    Season  TeamID        FGM        FGA      FGM3       FGA3        FTM  \\\n",
      "0     2003    1102  19.142857  39.785714  7.821429  20.821429  11.142857   \n",
      "1     2003    1103  27.148148  55.851852  5.444444  16.074074  19.037037   \n",
      "2     2003    1104  24.035714  57.178571  6.357143  19.857143  14.857143   \n",
      "3     2003    1105  24.384615  61.615385  7.576923  20.769231  15.423077   \n",
      "4     2003    1106  23.428571  55.285714  6.107143  17.642857  10.642857   \n",
      "5     2003    1107  24.035714  57.464286  7.928571  22.178571   9.928571   \n",
      "6     2003    1108  24.939394  58.727273  5.212121  16.333333  14.000000   \n",
      "7     2003    1110  23.166667  53.533333  8.766667  23.133333  11.266667   \n",
      "8     2003    1111  28.615385  64.500000  7.192308  21.961538  18.846154   \n",
      "9     2003    1112  30.321429  65.714286  7.035714  20.071429  17.535714   \n",
      "10    2003    1113  27.206897  56.896552  4.000000  12.586207  17.551724   \n",
      "11    2003    1114  23.857143  53.214286  6.785714  18.285714  15.535714   \n",
      "12    2003    1115  19.250000  54.285714  4.750000  16.857143  11.250000   \n",
      "13    2003    1116  21.821429  55.571429  6.071429  19.571429  11.928571   \n",
      "14    2003    1117  23.692308  54.692308  7.923077  21.346154  15.807692   \n",
      "\n",
      "          FTA         OR         DR        Ast         TO       Stl       Blk  \\\n",
      "0   17.107143   4.178571  16.821429  13.000000  11.428571  5.964286  1.785714   \n",
      "1   25.851852   9.777778  19.925926  15.222222  12.629630  7.259259  2.333333   \n",
      "2   20.928571  13.571429  23.928571  12.107143  13.285714  6.607143  3.785714   \n",
      "3   21.846154  13.500000  23.115385  14.538462  18.653846  9.307692  2.076923   \n",
      "4   16.464286  12.285714  23.857143  11.678571  17.035714  8.357143  3.142857   \n",
      "5   13.535714   8.250000  20.250000  11.928571  12.571429  6.857143  2.035714   \n",
      "6   20.939394  13.121212  23.212121  13.848485  18.454545  8.181818  3.515152   \n",
      "7   16.533333  10.000000  23.133333  14.233333  13.166667  6.400000  1.733333   \n",
      "8   26.192308  13.000000  25.307692  15.230769  16.192308  9.846154  4.615385   \n",
      "9   25.000000  15.178571  27.642857  17.642857  14.785714  8.464286  4.214286   \n",
      "10  26.206897  13.689655  23.310345  15.551724  14.000000  5.206897  4.241379   \n",
      "11  22.428571  10.535714  22.857143  11.500000  15.964286  6.607143  2.357143   \n",
      "12  17.607143   9.964286  21.035714  10.178571  19.107143  7.964286  2.750000   \n",
      "13  19.571429  14.535714  23.428571   9.821429  16.678571  6.464286  3.535714   \n",
      "14  21.423077  10.961538  20.384615  11.923077  13.269231  7.153846  1.653846   \n",
      "\n",
      "           PF  \n",
      "0   18.750000  \n",
      "1   19.851852  \n",
      "2   18.035714  \n",
      "3   20.230769  \n",
      "4   18.178571  \n",
      "5   15.892857  \n",
      "6   19.666667  \n",
      "7   18.133333  \n",
      "8   18.692308  \n",
      "9   17.750000  \n",
      "10  19.413793  \n",
      "11  20.500000  \n",
      "12  20.250000  \n",
      "13  20.928571  \n",
      "14  21.423077  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "post_season_outcomes['score_dif'] = post_season_outcomes.WScore - post_season_outcomes.LScore\n",
    "outcome = post_season_outcomes.loc[:,['Season', 'WTeamID', 'LTeamID', 'score_dif']]\n",
    "mixing_matrix = np.random.choice([True, False], len(outcome))\n",
    "mixed_outcome = outcome.copy()\n",
    "print(mixed_outcome.head())\n",
    "mixed_outcome.loc[mixing_matrix, ['WTeamID', 'LTeamID']] = mixed_outcome.loc[mixing_matrix, ['LTeamID', 'WTeamID']].values \n",
    "mixed_outcome.loc[mixing_matrix, ['score_dif']] = mixed_outcome.loc[mixing_matrix, ['score_dif']].mul(-1)\n",
    "print(mixed_outcome.head())\n",
    "print(len(mixed_outcome))\n",
    "features = pd.merge(mixed_outcome, team_summary_stats, left_on=['WTeamID', 'Season'], right_on=['TeamID', 'Season'])\n",
    "print(len(features))\n",
    "features = pd.merge(features, team_summary_stats, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], suffixes=('', '_t2'))\n",
    "print(len(features))\n",
    "features.loc[:, 'FGM':'PF'] = features.loc[:, 'FGM':'PF'].values - features.loc[:, 'FGM_t2':'PF_t2'].values\n",
    "features = features.loc[:, 'Season':'PF']\n",
    "features.to_csv('data/mixed_prepared_features.csv')\n",
    "print(features.head(15))\n",
    "print(team_summary_stats.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A possible model for predicting games based on regular season summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ModelFunctions import DecisionTreeFunc, KnnFunc, BayesianRidge, NeuralNetworkFunc\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore') # disables warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#features.score_dif = 1*(features.score_dif > 0)\n",
    "test_year = features.loc[features.Season == 2018, :]\n",
    "test_features = features.loc[features.Season != 2018, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KnnFunc(test_features.drop(columns=['score_dif', 'Season']), test_features.score_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 18.,   4., -12., -27.,  12.,  -9.,  -2.,  34.,  22.,  -2.,   2.,\n",
       "       -39.,  12.,  19.,   2., -26.,  -4.,   4.,  16.,  17.,  13.,  10.,\n",
       "       -10.,  33.,   8.,  -2.,  -2.,   5.,  -4.,  -4.,  -5.,   4.,  23.,\n",
       "        -8., -20., -17.,  12.,   3.,  35., -14.,  15., -39., -18.,  39.,\n",
       "       -10.,  23.,  -1.,   8.,  -1., -23.,  -9.,  15., -22.,   1., -28.,\n",
       "         9., -13.,   4.,   2.,   8.,  19.,  13.,   7.,   6.,  13.,   3.,\n",
       "        -5.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_year.drop(columns=['score_dif', 'Season']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Season  WTeamID  LTeamID  score_dif  TeamID        FGM        FGA  \\\n",
      "0    2003     1421     1411          8    1421  24.379310  56.793103   \n",
      "1    2003     1112     1436         29    1112  30.321429  65.714286   \n",
      "2    2003     1112     1211          1    1112  30.321429  65.714286   \n",
      "3    2003     1153     1211         -5    1153  22.892857  56.678571   \n",
      "4    2003     1112     1242         -3    1112  30.321429  65.714286   \n",
      "\n",
      "       FGM3       FGA3        FTM    ...        FGA3_t2     FTM_t2     FTA_t2  \\\n",
      "0  6.482759  18.000000  15.965517    ...      18.500000  17.400000  28.066667   \n",
      "1  7.035714  20.071429  17.535714    ...      15.482759  12.862069  19.551724   \n",
      "2  7.035714  20.071429  17.535714    ...      19.064516  17.774194  24.645161   \n",
      "3  6.678571  19.500000  14.857143    ...      19.064516  17.774194  24.645161   \n",
      "4  7.035714  20.071429  17.535714    ...      14.133333  16.066667  24.133333   \n",
      "\n",
      "       OR_t2      DR_t2     Ast_t2      TO_t2     Stl_t2    Blk_t2      PF_t2  \n",
      "0  13.166667  24.800000  14.200000  15.233333   6.433333  2.233333  18.300000  \n",
      "1  12.965517  25.724138  14.206897  14.068966   6.862069  2.965517  15.896552  \n",
      "2  11.935484  25.322581  15.741935  14.548387   6.806452  3.516129  18.645161  \n",
      "3  11.935484  25.322581  15.741935  14.548387   6.806452  3.516129  18.645161  \n",
      "4  14.300000  26.900000  16.733333  14.900000  10.133333  4.900000  16.700000  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('mlpregressor', MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_...=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None, param_grid={},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "mlp_reg = MLPRegressor()\n",
    "print(features.head())\n",
    "param_grid = {}#'mlpregressor__activation' : ['identity', 'logistic', 'tanh', 'relu']}\n",
    "#print(test_year.score_dif)\n",
    "pipe = make_pipeline(scaler, mlp_reg)\n",
    "grid = GridSearchCV(pipe, param_grid)\n",
    "grid.fit(test_features.drop(columns=['score_dif', 'Season']), test_features.score_dif)\n",
    "#print(grid.score(test_year.drop(columns=['score_dif', 'Season']), test_year.score_dif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real</th>\n",
       "      <th>predicted</th>\n",
       "      <th>FGAdif</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>-10</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.519886</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-3.531250</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>-15</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-2.687500</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>-4</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>5.833822</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>-2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.303030</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>18</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>1.596774</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>-3</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.387868</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>-21</td>\n",
       "      <td>34.0</td>\n",
       "      <td>-7.758467</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>20</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-7.199643</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>-22</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.409982</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>-4</td>\n",
       "      <td>-39.0</td>\n",
       "      <td>-7.515152</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>-25</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.814394</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.420304</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-15</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.386148</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-13</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>-8.225806</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-0.689150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.561670</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>4</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-0.439338</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.716912</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>-31</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.968750</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>-5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.931985</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>3</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-2.709447</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-7.785282</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-6.141098</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>-12</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-5.891544</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>-4</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>4.474383</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.294118</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>14</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-2.248577</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>1</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-1.946524</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.636364</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>13</td>\n",
       "      <td>35.0</td>\n",
       "      <td>-0.342246</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>-26</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-4.050710</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>-12</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-5.165775</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>-26</td>\n",
       "      <td>-39.0</td>\n",
       "      <td>-6.685662</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>-23</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>-6.323529</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>4</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.619960</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>17</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1.172014</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>15</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>11</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-5.954167</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>-10</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.701857</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>-7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.572825</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>6</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>1.219697</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>4</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-1.949198</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>4</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.591800</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>2</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>2.470588</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.126838</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>-16</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>3.535038</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>18</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.631527</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>5</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>-5.732008</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.568015</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.269795</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>-17</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-7.655172</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>12</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-2.470588</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>19</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.348485</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>-3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.167614</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.861193</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-5.142602</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.030303</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>16</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      real  predicted    FGAdif  correct\n",
       "981    -10       18.0  3.519886    False\n",
       "982      7        4.0 -3.531250     True\n",
       "983    -15      -12.0 -2.687500     True\n",
       "984     -4      -27.0  5.833822     True\n",
       "985     -2       12.0  2.303030    False\n",
       "986     18       -9.0  1.596774    False\n",
       "987     -3       -2.0  1.387868     True\n",
       "988    -21       34.0 -7.758467    False\n",
       "989     20       22.0 -7.199643     True\n",
       "990    -22       -2.0 -0.818182     True\n",
       "991      4        2.0 -1.409982     True\n",
       "992     -4      -39.0 -7.515152     True\n",
       "993    -25       12.0 -0.814394    False\n",
       "994      4       19.0  2.420304     True\n",
       "995    -15        2.0 -0.386148    False\n",
       "996    -13      -26.0 -8.225806     True\n",
       "997      2       -4.0 -0.689150    False\n",
       "998     16        4.0  1.561670     True\n",
       "999      4       16.0 -0.439338     True\n",
       "1000     4       17.0  4.716912     True\n",
       "1001   -31       13.0  5.968750    False\n",
       "1002    -5       10.0 -0.931985    False\n",
       "1003     3      -10.0 -2.709447    False\n",
       "1004     2       33.0 -7.785282     True\n",
       "1005     1        8.0 -6.141098     True\n",
       "1006   -12       -2.0 -5.891544     True\n",
       "1007    -4       -2.0  4.474383     True\n",
       "1008    17        5.0  4.294118     True\n",
       "1009    14       -4.0 -2.248577    False\n",
       "1010     1       -4.0 -1.946524    False\n",
       "...    ...        ...       ...      ...\n",
       "1018    10        3.0 -1.636364     True\n",
       "1019    13       35.0 -0.342246     True\n",
       "1020   -26      -14.0 -4.050710     True\n",
       "1021   -12       15.0 -5.165775    False\n",
       "1022   -26      -39.0 -6.685662     True\n",
       "1023   -23      -18.0 -6.323529     True\n",
       "1024     4       39.0  5.619960     True\n",
       "1025    17      -10.0  1.172014    False\n",
       "1026    15       23.0  0.625000     True\n",
       "1027    11       -1.0 -5.954167    False\n",
       "1028   -10        8.0  6.701857    False\n",
       "1029    -7       -1.0  3.572825     True\n",
       "1030     6      -23.0  1.219697    False\n",
       "1031     4       -9.0 -1.949198    False\n",
       "1032     4       15.0  1.591800     True\n",
       "1033     2      -22.0  2.470588    False\n",
       "1034    -1        1.0  9.126838    False\n",
       "1035   -16      -28.0  3.535038     True\n",
       "1036    18        9.0  4.631527     True\n",
       "1037     5      -13.0 -5.732008    False\n",
       "1038     4        4.0  2.568015     True\n",
       "1039    20        2.0  4.269795     True\n",
       "1040   -17        8.0 -7.655172    False\n",
       "1041    12       19.0 -2.470588     True\n",
       "1042    19       13.0  0.348485     True\n",
       "1043    -3        7.0  4.167614    False\n",
       "1044     5        6.0  2.861193     True\n",
       "1045     3       13.0 -5.142602     True\n",
       "1046    23        3.0  2.030303     True\n",
       "1047    16       -5.0  1.000000    False\n",
       "\n",
       "[67 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5970149253731343"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "results['real'] = test_year.score_dif\n",
    "results['predicted'] = model.predict(test_year.drop(columns=['score_dif', 'Season']))\n",
    "results['FGAdif'] = test_year.FGA\n",
    "\n",
    "results[results['real'] * results['predicted'] > 0]\n",
    "results['correct'] = results['real'] * results['predicted'] > 0\n",
    "display(results)\n",
    "len(results[results['real'] * results['predicted'] > 0]) / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0a8e4f540a09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
